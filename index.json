[{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"","tags":null,"title":"Andrew Marder","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":" If you\u0026rsquo;re new to Make, check out Mike Bostock\u0026rsquo;s article Why Use Make, it\u0026rsquo;s excellent! This post is intended as a follow-up to Mike\u0026rsquo;s introduction.\nI love Makefiles because they allow me to describe my workflow as a directed acyclic graph. Makefiles are a great example of declarative programming. When I specify a rule like the following:\ntargetfile: sourcefile command  I am saying that the targetfile depends on the sourcefile. Whenever I issue the command make targetfile, Make checks to see if anything in the targetfile\u0026rsquo;s dependency graph needs to be recompiled and it runs the necessary commands to bring the targetfile up to date. I enjoy using Make because it provides:\n A framework for writing reproducible research. A transparent caching mechanism. Often downloading data can take a lot of time, while cleaning data once it\u0026rsquo;s downloaded is relatively fast. By breaking these into two rules. I only need to download the data once and then I can focus on data cleaning and data analysis without re-running code from previous steps. A mechanism for building projects in parallel. Using make -j (or lsmake on the Grid) tells Make to run commands in parallel. All I have to specify is how each file in my project is built, Make figures out how to run everything in parallel.  Makefiles as Glue I often find myself using different tools for different jobs. I like using Python for web scraping, R for data visualization, and Stata for certain statistical models. Makefiles make it easy to combine different tools:\nDATA = data/processed/data.csv $(DATA): src/download.py python $\u0026lt; reports/figures/graph.pdf: src/graph.R $(DATA) Rscript $\u0026lt; reports/figures/table.tex: src/table.do $(DATA) stata-mp -b do $\u0026lt;  To understand the syntax above, read about variables and automatic variables.\nCompiling a Bunch of Files at Once Often the projects I work on require a lot of analyses. Imagine the following directory structure:\n. ├── Makefile ├── data │ └── processed │ └── data.dta └── src └── tables ├── table1.do ├── table2.do └── table3.do  Putting the following two rules in my Makefile allows me to recompile all tables with a single make tables command:\n%.log: %.do data/processed/data.dta cd $(dir $\u0026lt;); stata-mp -b do $(notdir $\u0026lt;) DO_FILES = $(shell find src/tables -name \u0026quot;*.do\u0026quot;) LOG_FILES = $(patsubst %.do,%.log,$(DO_FILES)) tables: $(LOG_FILES)  To understand the syntax above, read about pattern rules and functions.\nWorking with Databases Make cannot inspect when a database table was last modified. Imagine we have a script that updates a table of patent data. We can work this into a Makefile by creating a corresponding file to keep track of when the database table was last updated. A rule like the following will allow Make to keep track of when the patents table was last updated:\ndata/processed/patents.table: src/patents.py python $\u0026lt; echo \u0026quot;Data stored in PostgreSQL database.\u0026quot; \u0026gt; $@  Conclusion There are a crazy number of alternatives to Make. Here are just a few:\n Ant CMake Gradle Luigi Maven Ninja Rake SCons Waf  For the most part, I\u0026rsquo;ve found Make does everything I need it to do. Although the syntax is ugly, I appreciate how it ships with Unix-like operating systems (I find it annoying when I want to install a project and first I have to install the installation tool). That being said, I am very interested to experiment with Luigi (I\u0026rsquo;ve heard great things).\nIf you want to learn more about how I structure my projects, check out Cookiecutter Data Science.\n","date":1497830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1497830400,"objectID":"5d8b3da479f140b9491cd6a602861efe","permalink":"/post/make/","publishdate":"2017-06-19T00:00:00Z","relpermalink":"/post/make/","section":"post","summary":"If you\u0026rsquo;re new to Make, check out Mike Bostock\u0026rsquo;s article Why Use Make, it\u0026rsquo;s excellent! This post is intended as a follow-up to Mike\u0026rsquo;s introduction.\nI love Makefiles because they allow me to describe my workflow as a directed acyclic graph. Makefiles are a great example of declarative programming. When I specify a rule like the following:\ntargetfile: sourcefile command  I am saying that the targetfile depends on the sourcefile.","tags":["programming"],"title":"Makefile Tips","type":"post"},{"authors":null,"categories":null,"content":" A simulation for OLS model In an observational study, we need to assume we have the functional form to get causal effect estimated correctly, in addtion to the assumption of treatment being exogenous.\nlibrary(MASS) library(ggplot2) library(dplyr) library(tmle) library(glmnet) set.seed(366) nobs \u0026lt;- 2000 xw \u0026lt;- .8 xz \u0026lt;- .5 zw \u0026lt;- .6 nrow \u0026lt;- 3 ncol \u0026lt;- 3 covarMat = matrix( c(1^2, xz^2, xw^2, xz^2, 1^2, zw^2, xw^2, zw^2, 1^2 ) , nrow=ncol , ncol=ncol ) mu \u0026lt;- rep(0,3) rawvars \u0026lt;- mvrnorm(n=nobs, mu=mu, Sigma=covarMat) df \u0026lt;- tbl_df(rawvars) names(df) \u0026lt;- c(\u0026#39;x\u0026#39;,\u0026#39;z\u0026#39;,\u0026#39;w\u0026#39;) df \u0026lt;- df %\u0026gt;% mutate(log.x=log(x^2), log.z=log(z^2), log.w=log(w^2), z.sqr=z^2, w.sqr=w^2) %\u0026gt;% mutate(g.var= log.w + rnorm(nobs)) %\u0026gt;% mutate(A = rbinom(nobs, 1, 1/(1+exp((g.var))))) %\u0026gt;% mutate(y0=rnorm(nobs) + log.x) %\u0026gt;% mutate(tau.true = 2 + rnorm(nobs), y1=y0+tau.true, treat=A, y = treat*y1 + (1-treat)*y0) lm1 \u0026lt;- lm(y ~ A + log.w + log.x , data=df) summary(lm1) ## ## Call: ## lm(formula = y ~ A + log.w + log.x, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5506 -0.8372 -0.0154 0.8502 4.1624 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.01267 0.04778 0.265 0.791 ## A 1.93171 0.06580 29.355 \u0026lt;2e-16 *** ## log.w 0.01105 0.01428 0.774 0.439 ## log.x 1.00162 0.01353 74.030 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 1.255 on 1996 degrees of freedom ## Multiple R-squared: 0.758, Adjusted R-squared: 0.7576 ## F-statistic: 2084 on 3 and 1996 DF, p-value: \u0026lt; 2.2e-16 lm2 \u0026lt;- lm(y ~ A , data=df) summary(lm2) ## ## Call: ## lm(formula = y ~ A, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.9102 -1.3822 0.3241 1.6933 6.4046 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -0.92139 0.08992 -10.25 \u0026lt;2e-16 *** ## A 1.38964 0.11366 12.23 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 2.459 on 1998 degrees of freedom ## Multiple R-squared: 0.06961, Adjusted R-squared: 0.06915 ## F-statistic: 149.5 on 1 and 1998 DF, p-value: \u0026lt; 2.2e-16 lm3 \u0026lt;- lm(y ~ A + w, data=df) summary(lm3) ## ## Call: ## lm(formula = y ~ A + w, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.9112 -1.3795 0.3139 1.6863 6.3468 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -0.92056 0.08995 -10.234 \u0026lt;2e-16 *** ## A 1.38860 0.11368 12.215 \u0026lt;2e-16 *** ## w -0.03351 0.05286 -0.634 0.526 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 2.46 on 1997 degrees of freedom ## Multiple R-squared: 0.0698, Adjusted R-squared: 0.06887 ## F-statistic: 74.93 on 2 and 1997 DF, p-value: \u0026lt; 2.2e-16 lm4 \u0026lt;- lm(y ~ A + w + x, data=df) summary(lm4) ## ## Call: ## lm(formula = y ~ A + w + x, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.9109 -1.4047 0.3177 1.6926 6.4528 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -0.91759 0.08995 -10.201 \u0026lt;2e-16 *** ## A 1.38335 0.11372 12.164 \u0026lt;2e-16 *** ## w -0.09260 0.06827 -1.356 0.175 ## x 0.09947 0.07275 1.367 0.172 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 2.459 on 1996 degrees of freedom ## Multiple R-squared: 0.07067, Adjusted R-squared: 0.06927 ## F-statistic: 50.6 on 3 and 1996 DF, p-value: \u0026lt; 2.2e-16 In this example, treatment assignment process is determined by logged w, and outcome is dtermined by logged x and treatment. However, what we observe is w and x. In observational studies, this happens all the time. In fact, this is an ideal situation, that we observe variables that are determinants of outcome, although we are not sure about the functional form that determines the outcome. However, this example shows that unless we have observed exactly the factors themselves (in this case logged x, w, which determines the DGP), we have biased estimates of the true treatment effect.\nModel 1 is the only model with reasonable estimate of treatment effect (which is 2 in this case). Model 2 is a model with endogeneity: A is correlated with the missing variabel logged x. Model 3 and 4 we have x and w, but not logged, therefore still biased.\nThe lesson here is the functional form does matter. However, we have no way of knowing the functional form. What can we do here?\nQ.SL.library \u0026lt;- c(\u0026quot;SL.randomForest\u0026quot;, \u0026quot;SL.glmnet\u0026quot;,\u0026quot;SL.loess\u0026quot;,\u0026quot;SL.glm\u0026quot;,\u0026quot;SL.glm.interaction\u0026quot;, \u0026quot;SL.rpart\u0026quot;,\u0026quot;SL.nnet\u0026quot;,\u0026quot;SL.bayesglm\u0026quot;,\u0026quot;SL.gam\u0026quot;,\u0026quot;SL.gbm\u0026quot;,\u0026quot;SL.step\u0026quot;,\u0026quot;SL.mean\u0026quot;) g.SL.library \u0026lt;- c(\u0026quot;SL.randomForest\u0026quot;, \u0026quot;SL.glmnet\u0026quot;,\u0026quot;SL.glm\u0026quot;,\u0026quot;SL.glm.interaction\u0026quot;, \u0026quot;SL.rpart\u0026quot;,\u0026quot;SL.nnet\u0026quot;,\u0026quot;SL.bayesglm\u0026quot;,\u0026quot;SL.gam\u0026quot;,\u0026quot;SL.gbm\u0026quot;,\u0026quot;SL.step\u0026quot;,\u0026quot;SL.mean\u0026quot;) # this one is good since both Q and g are correct (including z in it) tmle1 \u0026lt;- tmle(Y = df$y, A = df$treat, W = df[,c(\u0026#39;x\u0026#39;,\u0026#39;w\u0026#39;)], g.SL.library = g.SL.library , Q.SL.library = Q.SL.library) tmle1 ## Additive Effect ## Parameter Estimate: 2.015 ## Estimated Variance: 0.0025985 ## p-value: \u0026lt;2e-16 ## 95% Conf Interval: (1.9151, 2.115) tmle2 \u0026lt;- tmle(Y = df$y, A = df$treat, W = df[,c(\u0026#39;x\u0026#39;,\u0026#39;w\u0026#39;, \u0026#39;z\u0026#39;)], g.SL.library = g.SL.library , Q.SL.library = Q.SL.library) tmle2 ## Additive Effect ## Parameter Estimate: 2.0263 ## Estimated Variance: 0.0029254 ## p-value: \u0026lt;2e-16 ## 95% Conf Interval: (1.9202, 2.1323) We use Mark van der Laan’s TMLE method. It uses SuperLearner as the initial estimator. It’s an ensemble of mulitple machine learning algorithms. Therefore it does not need to assume the functional form of the DGP. Even if we don’t have the variables that determines the DGP of outcome, if we observe some functions (even nonlinear functions) of these variables, we can still get reasonable estimates of the treatment effect.\nIn this example, we used multiple popular machine learning algorithms in modeling both treatment assingment process and the outcome process. The first TMLE model is with x and w (note not the logged x and w which are in the true DGP), the second one with an additional variable z.\nIt seems that TMLE results are less biased than the linear models with x and w. It may not be better than the linear model with logged x and w, but in empirical studies, we often cannot assume we have the variables in the DGP, but only some proxy of the variables in the DGP. I’ll do more simulations to see whether TMLE does perform better in the situation that we are not sure about the functional form. We should expect that is the case.\nSo far TMLE can only be used when treatment is binary variable.\nIt’s about time we embrace machine learning techniques into studies of caual effect in observational studies.\n ","date":1488326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488326400,"objectID":"7d2a7e62ae26d0228f02749e7cdefe70","permalink":"/post/2017-03-01-causal_tmle/","publishdate":"2017-03-01T00:00:00Z","relpermalink":"/post/2017-03-01-causal_tmle/","section":"post","summary":"A simulation for OLS model In an observational study, we need to assume we have the functional form to get causal effect estimated correctly, in addtion to the assumption of treatment being exogenous.\nlibrary(MASS) library(ggplot2) library(dplyr) library(tmle) library(glmnet) set.seed(366) nobs \u0026lt;- 2000 xw \u0026lt;- .8 xz \u0026lt;- .5 zw \u0026lt;- .6 nrow \u0026lt;- 3 ncol \u0026lt;- 3 covarMat = matrix( c(1^2, xz^2, xw^2, xz^2, 1^2, zw^2, xw^2, zw^2, 1^2 ) , nrow=ncol , ncol=ncol ) mu \u0026lt;- rep(0,3) rawvars \u0026lt;- mvrnorm(n=nobs, mu=mu, Sigma=covarMat) df \u0026lt;- tbl_df(rawvars) names(df) \u0026lt;- c(\u0026#39;x\u0026#39;,\u0026#39;z\u0026#39;,\u0026#39;w\u0026#39;) df \u0026lt;- df %\u0026gt;% mutate(log.","tags":["statistics","R"],"title":"Use machine learning for causal effect in observational study","type":"post"},{"authors":null,"categories":null,"content":" Interaction with two binary variables In a regression model with interaction term, people tend to pay attention to only the coefficient of the interaction term.\nLet’s start with the simpliest situation: \\(x_1\\) and \\(x_2\\) are binary and coded 0/1.\n\\[ E(y) = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1x_2 \\]\nIn this case, we have a saturated model; that is, we have three coefficients representing additive effects from the baseline situation (both \\(x_1\\) and \\(x_2\\) being 0). There are four different situations, with four combinations of \\(x_1\\) and \\(x_2\\).\nA lot of people just pay attention to the interaction term. In the case of studying treatment effects between two groups, say female and male, that makes sense, the interaction term representing the difference between male and female in terms of treatment effect.\nIn this model:\n\\[ E(y) = \\beta_1 female + \\beta_2 treatment + \\beta_{12} female*treatment \\]\nThe two dummy-coded binary variables, female and treatment, form four combinations. The following 2x2 table represents the expected means of the four cells(combinations).\n   male female    control \\(\\beta_0\\) \\(\\beta_0 + \\beta_1\\)  treatment \\(\\beta_0 + \\beta_2\\) \\(\\beta_0 + \\beta_1 + \\beta_2 + \\beta_{12}\\)    We can see from this table that, for example,\n\\[\\beta_0=E(Y|(0,0))\\]\nthat is, \\(\\beta_0\\) is the expected mean of the cell (0,0) (male and control).\n\\[\\beta_0 + \\beta_1 =E(Y|(1,0))\\]\nthat is ,\\(\\beta_0 + \\beta_1\\) is the expected mean of the cell (1,0) (female and control). And so on.\nNow,\n\\[ \\beta_{12} = (E(Y|(1,1))-E(Y|(0,1)))-(E(Y|(1,0))-E(Y|(0,0))) \\]\nthat is, the coefficient on the interaction term is actually the difference in difference. That’s why in many situations, people are only interested in the interaction coefficient, since they are only interested in the diff-in-diff estimates. The usually diff-in-diff estimator in causal inference literature refer to something similar, instead of female vs. male, people are interested in the treatment effect difference in before and after treatment. If we simply replace female/male dummy with before/after dummy, we can use the same logic. In those situations, it’s fine to mainly focus on the interaction term coefficient.\nIn some other situations, the three coefficients are equally important. It depends on your interest. For example, if we are interested in studying differences between union member and non-union member and black vs. non-black, we may not be only interested in the interaction effect. Instead, we might be interested in all four cells, maybe all possible pairwise comparisons. In that case, we should pay attention to all three coefficients. Stata’s “margins” command is of great help if we’d like to compare the cell means.\nLet’s take a look from a sample example in Stata:\nwebuse union3 reg ln_wage i.union##i.black, r margins union#black margins union#black, pwcompare  ## ## . webuse union3 ## (National Longitudinal Survey. Young Women 14-26 years of age in 1968) ## ## . reg ln_wage i.union##i.black, r ## ## Linear regression Number of obs = 1,244 ## F(3, 1240) = 34.76 ## Prob \u0026gt; F = 0.0000 ## R-squared = 0.0762 ## Root MSE = .37699 ## ## ------------------------------------------------------------------------------ ## | Robust ## ln_wage | Coef. Std. Err. t P\u0026gt;|t| [95% Conf. Interval] ## -------------+---------------------------------------------------------------- ## 1.union | .2045053 .0291682 7.01 0.000 .1472808 .2617298 ## 1.black | -.1709034 .0308067 -5.55 0.000 -.2313425 -.1104644 ## | ## union#black | ## 1 1 | .0386275 .0516609 0.75 0.455 -.062725 .13998 ## | ## _cons | 1.657525 .0138278 119.87 0.000 1.630396 1.684653 ## ------------------------------------------------------------------------------ ## ## . margins union#black ## ## Adjusted predictions Number of obs = 1,244 ## Model VCE : Robust ## ## Expression : Linear prediction, predict() ## ## ------------------------------------------------------------------------------ ## | Delta-method ## | Margin Std. Err. t P\u0026gt;|t| [95% Conf. Interval] ## -------------+---------------------------------------------------------------- ## union#black | ## 0 0 | 1.657525 .0138278 119.87 0.000 1.630396 1.684653 ## 0 1 | 1.486621 .027529 54.00 0.000 1.432613 1.54063 ## 1 0 | 1.86203 .0256822 72.50 0.000 1.811644 1.912415 ## 1 1 | 1.729754 .0325611 53.12 0.000 1.665873 1.793635 ## ------------------------------------------------------------------------------ ## ## . margins union#black, pwcompare ## ## Pairwise comparisons of adjusted predictions ## Model VCE : Robust ## ## Expression : Linear prediction, predict() ## ## ----------------------------------------------------------------- ## | Delta-method Unadjusted ## | Contrast Std. Err. [95% Conf. Interval] ## ----------------+------------------------------------------------ ## union#black | ## (0 1) vs (0 0) | -.1709034 .0308067 -.2313425 -.1104644 ## (1 0) vs (0 0) | .2045053 .0291682 .1472808 .2617298 ## (1 1) vs (0 0) | .0722294 .0353756 .0028268 .141632 ## (1 0) vs (0 1) | .3754087 .0376487 .3015466 .4492709 ## (1 1) vs (0 1) | .2431328 .0426388 .1594807 .326785 ## (1 1) vs (1 0) | -.1322759 .0414705 -.2136359 -.0509159 ## ----------------------------------------------------------------- ## ## . What we get by using “margins union#black” is the four cell means of \\(E(Y)\\), in this case, log of wage. Then “margins union#black, pwcompare” tells us all pairwise comparison of these four cell means. Instead of only paying attention to the interaction coefficient, in this case we might be interested in some comparisons of the four different situations of union and black. In fact, in this example, despite the interaction term being insignificant, all six comparisons of the cell means turn out to have 95% confidence intervals that do not include zero.\n Interaction with continuous variables Let’s start with the simpliest situation: \\(x_1\\) and \\(x_2\\) are continuous.\n\\[ E(y) = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1*x_2 \\]\nIn this case, we recommend “centering” \\(x_1\\) and \\(x_2\\) if they are continuous; that is, subtracting the mean value from each continuous independent variable when they are involved in the interaction term. There are two reason for it:\nTo reduce multi-collinearity. If the range of \\(x_1\\) and \\(x_2\\) include only positive numbers, then \\(x_1*x_2\\) can be highly correlated with both or one of \\(x_1\\) and \\(x_2\\). This can lead to numerical problems and unstable coefficient estimates (multi-collinearity problem).  “Centering” can reduce the correlation between the interaction term and the independent variables. If the original variables are normally distributed, interaction term after centering is actually uncorrelated with the original variables. When they are not normally distributed, centering will still reduce the correlation to a large degree.\nTo help with interpretation. In a model with interaction, \\(\\beta_1\\) represents the effect of \\(x_1\\) when \\(x_2\\) is zero. However, in many situations, zero is not within the range of \\(x_2\\). After centering, centered \\(x_2\\) at zero simply means original \\(x_2\\) at its mean value.  When we have dummy variable interacting with continuous variable, only continuous variable should be centered.\nAgain, Stata’s margins command is helpful.\nsysuse auto sum mpg gen mpg_centered=mpg-r(mean) sum mpg_centered reg price i.foreign##c.mpg_centered margins foreign, at(mpg_centered=(-3 (1) 3)) marginsplot graph export marginsplot.eps, replace ## ## . sysuse auto ## (1978 Automobile Data) ## ## . sum mpg ## ## Variable | Obs Mean Std. Dev. Min Max ## -------------+--------------------------------------------------------- ## mpg | 74 21.2973 5.785503 12 41 ## ## . gen mpg_centered=mpg-r(mean) ## ## . sum mpg_centered ## ## Variable | Obs Mean Std. Dev. Min Max ## -------------+--------------------------------------------------------- ## mpg_centered | 74 -4.03e-08 5.785503 -9.297297 19.7027 ## ## . reg price i.foreign##c.mpg_centered ## ## Source | SS df MS Number of obs = 74 ## -------------+---------------------------------- F(3, 70) = 9.48 ## Model | 183435285 3 61145094.9 Prob \u0026gt; F = 0.0000 ## Residual | 451630112 70 6451858.74 R-squared = 0.2888 ## -------------+---------------------------------- Adj R-squared = 0.2584 ## Total | 635065396 73 8699525.97 Root MSE = 2540.1 ## ## ------------------------------------------------------------------------------ ## price | Coef. Std. Err. t P\u0026gt;|t| [95% Conf. Interval] ## -------------+---------------------------------------------------------------- ## foreign | ## Foreign | 1666.519 717.217 2.32 0.023 236.0751 3096.963 ## mpg_centered | -329.2551 74.98545 -4.39 0.000 -478.8088 -179.7013 ## | ## foreign#| ## c. | ## mpg_centered | ## Foreign | 78.88826 112.4812 0.70 0.485 -145.4485 303.225 ## | ## _cons | 5588.295 369.0945 15.14 0.000 4852.159 6324.431 ## ------------------------------------------------------------------------------ ## ## . margins foreign, at(mpg_centered=(-3 (1) 3)) ## ## Adjusted predictions Number of obs = 74 ## Model VCE : OLS ## ## Expression : Linear prediction, predict() ## ## 1._at : mpg_centered = -3 ## ## 2._at : mpg_centered = -2 ## ## 3._at : mpg_centered = -1 ## ## 4._at : mpg_centered = 0 ## ## 5._at : mpg_centered = 1 ## ## 6._at : mpg_centered = 2 ## ## 7._at : mpg_centered = 3 ## ## ------------------------------------------------------------------------------ ## | Delta-method ## | Margin Std. Err. t P\u0026gt;|t| [95% Conf. Interval] ## -------------+---------------------------------------------------------------- ## _at#foreign | ## 1#Domestic | 6576.06 370.446 17.75 0.000 5837.229 7314.891 ## 1#Foreign | 8005.915 766.8178 10.44 0.000 6476.545 9535.284 ## 2#Domestic | 6246.805 354.4734 17.62 0.000 5539.83 6953.78 ## 2#Foreign | 7755.548 709.9327 10.92 0.000 6339.632 9171.464 ## 3#Domestic | 5917.55 354.0032 16.72 0.000 5211.513 6623.587 ## 3#Foreign | 7505.181 658.8306 11.39 0.000 6191.185 8819.177 ## 4#Domestic | 5588.295 369.0945 15.14 0.000 4852.159 6324.431 ## 4#Foreign | 7254.814 614.9548 11.80 0.000 6028.325 8481.303 ## 5#Domestic | 5259.04 397.981 13.21 0.000 4465.292 6052.788 ## 5#Foreign | 7004.447 579.9479 12.08 0.000 5847.778 8161.117 ## 6#Domestic | 4929.785 437.9413 11.26 0.000 4056.338 5803.231 ## 6#Foreign | 6754.081 555.4891 12.16 0.000 5646.192 7861.969 ## 7#Domestic | 4600.53 486.253 9.46 0.000 3630.729 5570.331 ## 7#Foreign | 6503.714 543.0057 11.98 0.000 5420.723 7586.704 ## ------------------------------------------------------------------------------ ## ## . marginsplot ## ## Variables that uniquely identify margins: mpg_centered foreign ## ## . graph export marginsplot.eps, replace ## (note: file marginsplot.eps not found) ## (file marginsplot.eps written in EPS format)  In this example, the graph shows the predicted price for foreign and domestic cars at different level of mpg.\n ","date":1487030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487030400,"objectID":"f6e975607c9db08f82c023656b63b451","permalink":"/post/2017-02-16-interpret_interaction/","publishdate":"2017-02-14T00:00:00Z","relpermalink":"/post/2017-02-16-interpret_interaction/","section":"post","summary":"Interaction with two binary variables In a regression model with interaction term, people tend to pay attention to only the coefficient of the interaction term.\nLet’s start with the simpliest situation: \\(x_1\\) and \\(x_2\\) are binary and coded 0/1.\n\\[ E(y) = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1x_2 \\]\nIn this case, we have a saturated model; that is, we have three coefficients representing additive effects from the baseline situation (both \\(x_1\\) and \\(x_2\\) being 0).","tags":["statistics","Stata"],"title":"Interpreting interaction term in a regression model","type":"post"},{"authors":null,"categories":null,"content":" Marginal effects in a linear model Stata’s margins command has been a powerful tool for many economists. It can calculate predicted means as well as predicted marginal effects. However, we do need to be careful when we use it when fixed effects are included. In a linear model, everything works out fine. However, in a non-linear model, you may not want to use margins, since it’s not calculating what you have in mind.\nIn a linear model with fixed effects, we can do it either by “demeaning” every variable, or include dummy variables. They return the same results. Fortunately, marginal effects can be calculated the same way in both models.\nFor example:\nclear sysuse auto xtset rep78 xtreg price c.mpg##c.trunk, fe margins , dydx(mpg) reg price c.mpg##c.trunk i.rep78 margins , dydx(mpg) ## ## . clear ## ## . sysuse auto ## (1978 Automobile Data) ## ## . xtset rep78 ## panel variable: rep78 (unbalanced) ## ## . xtreg price c.mpg##c.trunk, fe ## ## Fixed-effects (within) regression Number of obs = 69 ## Group variable: rep78 Number of groups = 5 ## ## R-sq: Obs per group: ## within = 0.2570 min = 2 ## between = 0.0653 avg = 13.8 ## overall = 0.2237 max = 30 ## ## F(3,61) = 7.03 ## corr(u_i, Xb) = -0.4133 Prob \u0026gt; F = 0.0004 ## ## ------------------------------------------------------------------------------ ## price | Coef. Std. Err. t P\u0026gt;|t| [95% Conf. Interval] ## -------------+---------------------------------------------------------------- ## mpg | -98.12003 226.8708 -0.43 0.667 -551.7763 355.5362 ## trunk | 295.0544 343.3934 0.86 0.394 -391.6032 981.712 ## | ## c.mpg#| ## c.trunk | -12.23318 15.94713 -0.77 0.446 -44.12143 19.65506 ## | ## _cons | 7574.85 5321.325 1.42 0.160 -3065.797 18215.5 ## -------------+---------------------------------------------------------------- ## sigma_u | 992.2156 ## sigma_e | 2631.2869 ## rho | .12449059 (fraction of variance due to u_i) ## ------------------------------------------------------------------------------ ## F test that all u_i=0: F(4, 61) = 0.86 Prob \u0026gt; F = 0.4948 ## ## . margins , dydx(mpg) ## ## Average marginal effects Number of obs = 69 ## Model VCE : Conventional ## ## Expression : Linear prediction, predict() ## dy/dx w.r.t. : mpg ## ## ------------------------------------------------------------------------------ ## | Delta-method ## | dy/dx Std. Err. z P\u0026gt;|z| [95% Conf. Interval] ## -------------+---------------------------------------------------------------- ## mpg | -268.4981 74.12513 -3.62 0.000 -413.7807 -123.2156 ## ------------------------------------------------------------------------------ ## ## . reg price c.mpg##c.trunk i.rep78 ## ## Source | SS df MS Number of obs = 69 ## -------------+---------------------------------- F(7, 61) = 3.19 ## Model | 154453046 7 22064720.8 Prob \u0026gt; F = 0.0061 ## Residual | 422343913 61 6923670.71 R-squared = 0.2678 ## -------------+---------------------------------- Adj R-squared = 0.1838 ## Total | 576796959 68 8482308.22 Root MSE = 2631.3 ## ## ------------------------------------------------------------------------------ ## price | Coef. Std. Err. t P\u0026gt;|t| [95% Conf. Interval] ## -------------+---------------------------------------------------------------- ## mpg | -98.12003 226.8708 -0.43 0.667 -551.7763 355.5362 ## trunk | 295.0544 343.3934 0.86 0.394 -391.6032 981.712 ## | ## c.mpg#| ## c.trunk | -12.23318 15.94713 -0.77 0.446 -44.12143 19.65506 ## | ## rep78 | ## 2 | 438.0002 2161.922 0.20 0.840 -3885.031 4761.031 ## 3 | 987.1363 2022.606 0.49 0.627 -3057.315 5031.587 ## 4 | 1240.944 2046.417 0.61 0.547 -2851.12 5333.008 ## 5 | 2605.83 2161.837 1.21 0.233 -1717.031 6928.691 ## | ## _cons | 6355.731 5209.899 1.22 0.227 -4062.105 16773.57 ## ------------------------------------------------------------------------------ ## ## . margins , dydx(mpg) ## ## Average marginal effects Number of obs = 69 ## Model VCE : OLS ## ## Expression : Linear prediction, predict() ## dy/dx w.r.t. : mpg ## ## ------------------------------------------------------------------------------ ## | Delta-method ## | dy/dx Std. Err. t P\u0026gt;|t| [95% Conf. Interval] ## -------------+---------------------------------------------------------------- ## mpg | -268.4981 74.12513 -3.62 0.001 -416.7205 -120.2758 ## ------------------------------------------------------------------------------ All is fine.\n Marginal effects in a non-linear model In a nonlinear model, we need to be more careful:\nclear sysuse auto xtset rep78 xtpoisson price mpg trunk, fe margins , dydx(mpg) margins , dydx(mpg) predict(nu0) poisson price mpg trunk i.rep78 margins , dydx(mpg)  ## ## . clear ## ## . sysuse auto ## (1978 Automobile Data) ## ## . xtset rep78 ## panel variable: rep78 (unbalanced) ## ## . xtpoisson price mpg trunk, fe ## ## Iteration 0: log likelihood = -39282.052 ## Iteration 1: log likelihood = -27527.055 ## Iteration 2: log likelihood = -27518.944 ## Iteration 3: log likelihood = -27518.944 ## ## Conditional fixed-effects Poisson regression Number of obs = 69 ## Group variable: rep78 Number of groups = 5 ## ## Obs per group: ## min = 2 ## avg = 13.8 ## max = 30 ## ## Wald chi2(2) = 22890.68 ## Log likelihood = -27518.944 Prob \u0026gt; chi2 = 0.0000 ## ## ------------------------------------------------------------------------------ ## price | Coef. Std. Err. z P\u0026gt;|z| [95% Conf. Interval] ## -------------+---------------------------------------------------------------- ## mpg | -.0450221 .0003814 -118.05 0.000 -.0457696 -.0442746 ## trunk | .0047349 .0004772 9.92 0.000 .0037996 .0056702 ## ------------------------------------------------------------------------------ ## ## . margins , dydx(mpg) ## ## Average marginal effects Number of obs = 69 ## Model VCE : OIM ## ## Expression : Linear prediction, predict() ## dy/dx w.r.t. : mpg ## ## ------------------------------------------------------------------------------ ## | Delta-method ## | dy/dx Std. Err. z P\u0026gt;|z| [95% Conf. Interval] ## -------------+---------------------------------------------------------------- ## mpg | -.0450221 .0003814 -118.05 0.000 -.0457696 -.0442746 ## ------------------------------------------------------------------------------ ## ## . margins , dydx(mpg) predict(nu0) ## ## Average marginal effects Number of obs = 69 ## Model VCE : OIM ## ## Expression : Predicted number of events (assuming u_i=0), predict(nu0) ## dy/dx w.r.t. : mpg ## ## ------------------------------------------------------------------------------ ## | Delta-method ## | dy/dx Std. Err. z P\u0026gt;|z| [95% Conf. Interval] ## -------------+---------------------------------------------------------------- ## mpg | -.0190939 .0001245 -153.35 0.000 -.0193379 -.0188498 ## ------------------------------------------------------------------------------ ## ## . poisson price mpg trunk i.rep78 ## ## Iteration 0: log likelihood = -27550.942 ## Iteration 1: log likelihood = -27550.912 ## Iteration 2: log likelihood = -27550.912 ## ## Poisson regression Number of obs = 69 ## LR chi2(6) = 24962.86 ## Prob \u0026gt; chi2 = 0.0000 ## Log likelihood = -27550.912 Pseudo R2 = 0.3118 ## ## ------------------------------------------------------------------------------ ## price | Coef. Std. Err. z P\u0026gt;|z| [95% Conf. Interval] ## -------------+---------------------------------------------------------------- ## mpg | -.0450221 .0003814 -118.05 0.000 -.0457696 -.0442746 ## trunk | .0047349 .0004772 9.92 0.000 .0037996 .0056702 ## | ## rep78 | ## 2 | .1476657 .0117935 12.52 0.000 .1245509 .1707805 ## 3 | .2295466 .0111741 20.54 0.000 .2076458 .2514474 ## 4 | .2726354 .0112656 24.20 0.000 .2505552 .2947155 ## 5 | .4682657 .0115137 40.67 0.000 .4456992 .4908321 ## | ## _cons | 9.323117 .0149274 624.57 0.000 9.29386 9.352374 ## ------------------------------------------------------------------------------ ## ## . margins , dydx(mpg) ## ## Average marginal effects Number of obs = 69 ## Model VCE : OIM ## ## Expression : Predicted number of events, predict() ## dy/dx w.r.t. : mpg ## ## ------------------------------------------------------------------------------ ## | Delta-method ## | dy/dx Std. Err. z P\u0026gt;|z| [95% Conf. Interval] ## -------------+---------------------------------------------------------------- ## mpg | -276.7079 2.382193 -116.16 0.000 -281.3769 -272.0389 ## ------------------------------------------------------------------------------ ## ## . In this example, “xtpoisson, fe” and “poisson i.rep78” returns the same results. Fixed effect Poisson model (sometimes called conditional fixed effect Poisson) is the same models as a Poisson model with dummies, just like a linear model (OLS with dummies is the same as fixed effect OLS). Poisson model and OLS are unique in this sense that there is no “incidental paramater” problem.\nWe see in this example, margins commands do not return the same marginal effects, even though the models are the same. The reason behind this is that in a conditional fixed effect Poisson, the fixed effects are not estimated (they are not in the final likelihood function that gets estimated). Therefore, we’ll have to make a decision what values to use as the values of the fixed effects. “margins, predict(nu0)” simply set all fixed effects to zero. On the other hand, margins after Poisson model with dummies does not do that. The fixed effect in that case gets estimated. Therefore the marginal effects in that case make more sense.\nSo our advise for a conditioanl Poisson model is that we should not use margins to calculate marginal effects afterwards; instead, we should simply stick with the original coefficient estimates.\nThe same logic applies to the conditional logit model. Fixed effects are not estimated in that model; simply setting them to zero does not make too much sense. In addition, conditional logit model is not the same model as a logit model with dummies, since there is the “incidental paramater” problem. Again, we should just focus on the coefficient estimates as the effect on the logged odds.\n ","date":1487030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487030400,"objectID":"d3844ef813085e962a57c0924e2d476e","permalink":"/post/2017-02-16-margins_nonlinear/","publishdate":"2017-02-14T00:00:00Z","relpermalink":"/post/2017-02-16-margins_nonlinear/","section":"post","summary":"Marginal effects in a linear model Stata’s margins command has been a powerful tool for many economists. It can calculate predicted means as well as predicted marginal effects. However, we do need to be careful when we use it when fixed effects are included. In a linear model, everything works out fine. However, in a non-linear model, you may not want to use margins, since it’s not calculating what you have in mind.","tags":["statistics","Stata"],"title":"Marginal effects in models with fixed effects","type":"post"},{"authors":null,"categories":null,"content":" In empirical studies, data sets with a lot of zeros are often hard to model. There are various models to deal with it: zero-inflated Poisson model, Negative Binomial (NB)model, hurdle model, etc.\nHere we are following a zero-inflated model’s thinking: model the data with two processes. One is a Bernoulli process, the other one is a count data process (Poisson or NB).\nWe’d like to see, in this simulation exercise, how different models perform with changes of sample size and percentage of zeros (we expect the less zero, the better a plain Poisson model would perform). Therefore we vary sample size \\(n\\) and an indicator of how much percentage of zeros in the data \\(\\theta\\).\nFor the count data process (\\(y_c\\)):\n\\[ log(y_c) = 2 x + u \\]\nFor the Bernoulli process (\\(y_b\\)):\n\\[ z_1 = 4 z + \\theta \\]\n\\[ logit(y_b) = z_1 \\]\n\\[ p_y = \\frac{e^{z_1}}{1+e^{z_1}} \\]\nCombining these two processes:\n\\[ y = y_c \\ \\text{if} \\ p_y=1 \\]\n\\[ y = y_b \\ \\text{if} \\ p_y=0 \\]\nZero-inflated Poisson models A zero-inflated Poisson needs specifying both the binary process and the count process correctly. Often than not, we don’t have a model for the binary process. Many people simply use the same explanatory variables for both processes. We simulate both situations. Case 1: suppose we observe \\(z\\), and case 2: suppose we don’t observe \\(z\\). In the graph below, they are labeled zip1 and zip2.\n Poisson model A plain Poisson model returns a consistent estimator for the coefficients, with or without Poisson-distributed data. We expect Poisson model’s performance improve with sample size. Note that the standard errors from a Poisson model needs adjustment, which we do not discuss in this post.\n NB model NB model is used widely to handle “overdispersion” problem. That is, the variance far exceeds the mean, therefore the Poisson model is considered inappropriate. NB model addresses that by allowing an extra parameter. However, many people also use it to model “extra zero” situation, we’ll see in our simulation it may not be better than a plain Poisson model.\n Log-linear model What about an OLS model with \\(log(y+1)\\)?\n hurdle model A hurdle model models the zero’s and other values separately; that is, the zero’s are from a binomial process only, the other positive values are from a truncated count data process. We assume here, in the simulation, we don’t observe \\(z\\). Therefore, \\(x\\) is determining both binary and count processes. In the graph below, it’s labeled hurdle.\n````\nCount data models can be used even if data is not “counts”; for example, some positive non-integer numbers. In fact, Poisson model is consistent even if data is not Poisson-distributed, if the model specification is correct on modeling the log of expected counts. We simulate both scenarios: Case 1, data is generated from a Poisson process. Case 2, data is generated from a Normal distribution, but we use count data models to model it. The above code is for case 2.\nWe simulate 100 times with \\(\\theta\\) ranging from -4 to 4, lower number means higher percentage of zeros; number of observations from \\(e^4\\) to \\(e^9\\).\nSince there are many simulations, we used “snowfall” library to speed things up.\nFor raw code, please visit case1: poisson and case2: normal.\n      In the graph, there are two vertical lines. The lighter one is the bias, the other one is MSE.\nIf we can compare the situations that data generated from Poisson process and normal process, we can see using count data models to model normal distributed data is still valid, just with bigger standard deviations. With large sample, actually Poisson model out-performs NB, and Log-linear model, without having to model the extra zeros. NB model does not do well, in general. Log-linear model is the worst. Zero-inflated Poisson with correct specification of the binary process performs the best, naturally. But that relies on correct specification of the binary process, which is not always realistic. Zero-inflated Poisson or hurdle model without correct specification of the binary process are not too bad, especially when sample size is large. These two are very close since only the difference between the two is that hurdle is modeling all zeros from binary process and all positive numbers from count data process; while zip2 is modeling some zeros (probably most) from binary process and all other values (including some zeros) from a Poisson process.\n ","date":1410912000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1410912000,"objectID":"43c78dc97c3771adc6b64e06c739aff0","permalink":"/post/2014-09-17-poisson-models/","publishdate":"2014-09-17T00:00:00Z","relpermalink":"/post/2014-09-17-poisson-models/","section":"post","summary":"In empirical studies, data sets with a lot of zeros are often hard to model. There are various models to deal with it: zero-inflated Poisson model, Negative Binomial (NB)model, hurdle model, etc.\nHere we are following a zero-inflated model’s thinking: model the data with two processes. One is a Bernoulli process, the other one is a count data process (Poisson or NB).\nWe’d like to see, in this simulation exercise, how different models perform with changes of sample size and percentage of zeros (we expect the less zero, the better a plain Poisson model would perform).","tags":["statistics"],"title":"A comparison of various count data models with extra zeros","type":"post"},{"authors":null,"categories":null,"content":" This post is inspired by diffuse prior\nLewbel’s 2012 paper proposed an estimator based on heteroscedasticity to address the problem of endogeneity without an instrument. This problem has been an issue for many (maybe most) empirical researchers with observational data. People are challenged with endogeneity and they have difficulty locating a valid instrument (who doesn’t?).\nUsing the “ivlewbel” package in R, I compare the performance of Lewbel’s estimator with OLS and TSLS (two stage least square) estimators, with different values of sample size, and heteroscedasticity.\nlibrary(ivlewbel) require(snowfall) set.seed(666) ## initialize parallel cores. sfInit( parallel=TRUE, cpus=12) gen.sim \u0026lt;- function(df){ k \u0026lt;- df[\u0026#39;k\u0026#39;] nobs \u0026lt;- df[\u0026#39;nobs\u0026#39;] x\u0026lt;-runif(nobs, min=-1, max=1) u \u0026lt;- rnorm(nobs,0,1) u1 \u0026lt;- rnorm(nobs,0,1) u2 \u0026lt;- rnorm(nobs,0,1) x1 \u0026lt;-runif(nobs, min=-1, max=1) x2 \u0026lt;- rnorm(nobs,0,1) z \u0026lt;- rnorm(nobs,0,1) e1 = exp(.3*k*(x+x1))*u1 e2 = u2 ## y1 is the endogenous variable; z is the instrument; x1 is ## omitted but determines heteroskedasticity of y1; e1 e2 are ## correlated because of common factor of u; x is the only ## observed exogenous variable. The true coefficient on y1 should ## be 1. lewbel model use x as both the exogenous variable and ## the heteroscedasticity factor. tsls assumes we have an ## instrument z. k is to adjust for degree of heteroscedasticity. y1 = 1 + z + x + x1 + e1 y2 = 1 + y1 + x + x1 + e2 data = data.frame(y2, y1, x1, x2, z, x) lewbel.model \u0026lt;- lewbel(formula = y2 ~ y1 | x | x , data = data) lm.model \u0026lt;- lm(y2 ~ y1 + x, data=data) tsls.model \u0026lt;- tsls(y2 ~ y1 + x , ~ z + x , data=data) lm.y1 \u0026lt;- summary(lm.model)$coefficients[\u0026#39;y1\u0026#39;,\u0026#39;Estimate\u0026#39;]-1 tsls.y1 \u0026lt;- tsls.model$coefficients[\u0026#39;y1\u0026#39;]-1 lewbel.y1 \u0026lt;- lewbel.model$coef.est[\u0026#39;y1\u0026#39;, \u0026#39;Estimate\u0026#39;]-1 return(c(lm=lm.y1, lewbel=lewbel.y1,tsls=tsls.y1)) } ## set parameter space sim.grid = seq(1,100,1) k.grid=seq(1,10,1) nobs.grid = ceiling(exp(seq(4, 8, 1))/100)*100 data.grid \u0026lt;- expand.grid(nobs.grid, sim.grid, k.grid) names(data.grid) \u0026lt;- c(\u0026#39;nobs\u0026#39;, \u0026#39;nsim\u0026#39;, \u0026#39;k\u0026#39;) ## export functions to the slaves ## export data to the slaves if necessary sfExport(list=list(\u0026quot;gen.sim\u0026quot;)) ## export function to the slaves sfLibrary(ivlewbel) ## parallel computing results \u0026lt;- data.frame(t(sfApply(data.grid, 1, gen.sim))) ## stop the cluster sfStop() names(results) \u0026lt;- c(\u0026#39;lm\u0026#39;,\u0026#39;lewbel\u0026#39;,\u0026#39;tsls\u0026#39;) forshiny \u0026lt;- cbind(data.grid, results) ## write out for use in shiny. ## write.csv(forshiny, \u0026#39;results.csv\u0026#39;) The data generating processes in this simulation study are: \\[y_2\\] is the dependent variable.\n\\[ y_2 = y_1 + x + x_1 + e_2 \\]\n\\[ y_1 = z + x + x_1 + e_1 \\]\nHere \\[e_1\\] and \\[e_2\\] are the error terms. Among the independent variables of \\[y_2\\], \\[x\\] is observed; \\[x_1\\] is unobserved, \\[z\\] is the intended instruments. \\[y_1\\] is the endogenous variable, since it’s determined by \\[x_1\\], and \\[x_1\\] is part of \\[y_2\\]’s error term since \\[x_1\\] is unobserved. If we have \\[z\\], then we can use TSLS to estimate the model. If not, then we’ll try Lewbel’s model to see if it works.\nThe ‘gen_sim’ function returns the three estimates (OLS, TSLS and Lewbel). \\[e_1\\] is assumed to have some degree of heteroscedasticity:\n\\[ e_1 = e^{0.3*k*(x+x_1)}*u_1 \\]\nwhere \\[u_1\\] is a standard normal variable. \\[k\\] is a variable used to adjust for degree of heteroscedasticity. Here we assume we know a variable that determines the heteroscedasticity: \\[x\\]. But \\[x_1\\] remains unobserved.\nWe then simulate 100 times with \\[k\\] ranging from 1 to 10; number of observations from \\[e^4\\] to \\[e^8\\].\nSince there are many simulations, we used “snowfall” library to speed things up.\nFor raw code, please visit here.\n   We can see at moderate degree of heteroscedasticity, Lewbel’s estimator performs well, at reasonably large sample size. TSLS performs well since we assume we observe \\[z\\]. At very high degree of heteroscedasticity, both OLS and Lewbel’s estimator perform well. My explanation is that when there is very high degree of heteroscedasticity, heteroscedasticity just outplays endogeneity so that OLS’ bias goes down (since we know that OLS under heteroscedasticity is consistent.), with large sample size.\n","date":1410480000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1410480000,"objectID":"4377403d4aea3157c73031f9c008396f","permalink":"/post/2014-09-12-lewbel-vs-ols-vs-tsls/","publishdate":"2014-09-12T00:00:00Z","relpermalink":"/post/2014-09-12-lewbel-vs-ols-vs-tsls/","section":"post","summary":"This post is inspired by diffuse prior\nLewbel’s 2012 paper proposed an estimator based on heteroscedasticity to address the problem of endogeneity without an instrument. This problem has been an issue for many (maybe most) empirical researchers with observational data. People are challenged with endogeneity and they have difficulty locating a valid instrument (who doesn’t?).\nUsing the “ivlewbel” package in R, I compare the performance of Lewbel’s estimator with OLS and TSLS (two stage least square) estimators, with different values of sample size, and heteroscedasticity.","tags":["statistics"],"title":"A comparison of Lewbel model vs. OLS and TSLS","type":"post"}]